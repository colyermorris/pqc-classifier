pqc-classifier/
├─ README.md
├─ CHANGELOG.md
├─ LICENSE
├─ .gitignore
├─ config/
│  ├─ project.yaml                 # roots, environment switches, tags
│  ├─ paths.yaml                   # all canonical file paths used in code & docs
│  ├─ secrets.example.env          # never commit real secrets
├─ docs/
│  ├─ proposal.md                  # source of truth (your Claude doc distilled)
│  ├─ charter.md                   # scope, objectives, out-of-scope
│  ├─ timeline.md                  # milestones & burndown
│  ├─ risk-register.md
│  ├─ ethics.md
│  ├─ data-governance.md
│  ├─ dataset-limitations.md       # (explicitly captures current limits & plan to expand)
│  ├─ model-cards/
│  │  └─ family_classifier.md
│  │  └─ level_classifier.md
│  └─ api-spec.md                  # REST/GraphQL surface for dashboard
├─ data/
│  ├─ raw/
│  │  ├─ pqclean/                  # api.h trees
│  │  ├─ oqs/                      # liboqs outputs, perf logs
│  │  ├─ literature/               # extracted tables, PDFs (metadata only)
│  │  └─ industry/                 # partner drops (anonymized)
│  ├─ interim/
│  │  └─ staging/                  # lightly cleaned, pre-schema
│  ├─ processed/
│  │  ├─ master_catalog.parquet
│  │  ├─ features_static.parquet
│  │  ├─ features_dynamic.parquet
│  │  └─ features_merged.parquet
│  └─ schemas/
│     └─ records.schema.json
├─ src/
│  ├─ ingest/
│  │  ├─ pqclean_ingest.py
│  │  ├─ oqs_ingest.py
│  │  ├─ literature_ingest.py
│  │  └─ industry_ingest_template.md
│  ├─ validate/
│  │  ├─ validators.py
│  │  └─ canonical_names.yaml      # alias mapping across sources
│  ├─ features/
│  │  ├─ build_static_features.py
│  │  ├─ build_dynamic_features.py
│  │  └─ feature_registry.yaml
│  ├─ models/
│  │  ├─ train_baselines.py
│  │  ├─ train_advanced.py
│  │  ├─ model_registry.yaml
│  │  └─ params/
│  │     ├─ logreg.yaml
│  │     ├─ rf.yaml
│  │     └─ lgbm.yaml
│  ├─ eval/
│  │  ├─ cross_validation.py
│  │  ├─ robustness.py
│  │  ├─ calibration.py
│  │  └─ reports/                  # HTML/MD outputs
│  ├─ interpret/
│  │  ├─ shap_analysis.py
│  │  └─ lime_analysis.py
│  ├─ api/
│  │  ├─ service.py                # FastAPI stub
│  │  └─ schemas.py
│  └─ utils/
│     ├─ io.py
│     └─ logging.py
├─ dashboard/
│  ├─ backend/                     # FastAPI wrapper if split from src/api
│  │  └─ requirements.txt
│  └─ frontend/                    # React/TypeScript
│     ├─ package.json
│     └─ src/
├─ scripts/
│  ├─ make_dataset.sh
│  ├─ run_cv.sh
│  └─ generate_reports.sh
├─ infra/
│  ├─ Dockerfile                   # monolith dev image
│  ├─ docker-compose.yml
│  └─ k8s/
│     ├─ api-deployment.yaml
│     └─ dashboard-deployment.yaml
└─ ci/
   └─ github/
      └─ workflows/
         ├─ lint-test.yml
         ├─ data-pipeline.yml
         └─ model-train.yml


================================================================
[WP-00] BOOTSTRAP REPO & GOVERNANCE
GOAL
Create the scaffolding, config, and initial docs so everything downstream has known paths and terms.
INPUTS
Proposal text (this document).
License choice (MIT/Apache-2/etc).
Repo name.
FILES TO CREATE/UPDATE
README.md, .gitignore, LICENSE, CHANGELOG.md
config/project.yaml, config/paths.yaml, config/secrets.example.env
docs/proposal.md, docs/charter.md, docs/timeline.md
docs/risk-register.md, docs/ethics.md, docs/data-governance.md
docs/dataset-limitations.md
docs/model-cards/family_classifier.md
docs/model-cards/level_classifier.md
ACTIONS
Initialize git repo, commit the skeleton.
In config/paths.yaml, define absolute/relative roots for data/, src/, docs/, dashboard/.
In docs/dataset-limitations.md, explicitly capture current dataset limits and the plan to expand post-training.
DELIVERABLES
Clean repo with all files above.
README.md shows repo map and the one-liner commands you’ll run later (scripts/*).
ACCEPTANCE CHECK
All paths in config/paths.yaml resolve locally.
docs/dataset-limitations.md lists: sample size, benchmark bias, missing dynamic features, environment diversity, and a scheduled refresh plan.
NOTES
(Your notes here)
================================================================
[WP-01] DATA ACCESS & PROVENANCE GUARDRAILS
GOAL
Decide what we ingest, how we store it, and how we track provenance/licensing.
FILES TO CREATE/UPDATE
docs/data-governance.md
data/schemas/records.schema.json
src/validate/validators.py (function names only for now)
ACTIONS
Write records.schema.json fields: source, family, algorithm, nist_level, impl_variant, platform, sizes (key/ciphertext/signature), timings, mem, ops_sec, latency_ms, citation_key, license_tag, provenance.
In docs/data-governance.md, add intake checklist and retention policy.
DELIVERABLES
Example toy record at docs/examples/record.json (validates).
ACCEPTANCE CHECK
Schema validator passes for the toy record.
docs/data-governance.md includes an intake checklist and anonymization rules.
NOTES
(Your notes here)
================================================================
[WP-02] CANONICAL NAMING & ALIAS MAP
GOAL
Normalize names across sources so joins are clean and repeatable.
FILES TO CREATE/UPDATE
src/validate/canonical_names.yaml
docs/dataset-limitations.md (aliasing section)
ACTIONS
Map common aliases for algorithms and families (e.g., kyber, crystals-kyber -> ML-KEM).
Add families {lattice, code, hash} and security level naming (NIST level integers).
DELIVERABLES
Alias coverage table appended to docs/dataset-limitations.md#alias-coverage.
ACCEPTANCE CHECK
≥ 50 aliases resolved; all families represented.
NOTES
(Your notes here)
================================================================
[WP-03] INGEST PQCLEAN (api.h → records)
GOAL
Parse PQClean api.h trees into schema-conformant records.
FILES TO CREATE/UPDATE
src/ingest/pqclean_ingest.py
data/raw/pqclean/ (drop source here)
scripts/make_dataset.sh (add a make: pqclean target name for later)
ACTIONS
Extract sizes (key/ciphertext/signature) and parameter set identifiers.
Attach canonical names via src/validate/canonical_names.yaml.
DELIVERABLES
data/interim/staging/pqclean_records.jsonl
ACCEPTANCE CHECK
All PQClean algs present; size fields populated; schema valid.
NOTES
(Your notes here)
================================================================
[WP-04] INGEST OQS (bench logs → records)
GOAL
Convert liboqs bench results into timing/memory/platform fields.
FILES TO CREATE/UPDATE
src/ingest/oqs_ingest.py
data/raw/oqs/
data/schemas/records.schema.json (ensure timing/memory fields exist)
ACTIONS
Parse throughput/latency, CPU model, memory, implementation variant (ref/opt/hw).
Map algorithm names via canonical_names.yaml.
DELIVERABLES
data/interim/staging/oqs_records.jsonl
ACCEPTANCE CHECK
Records for ≥ 3 families and ≥ 2 security levels include timing + platform fields.
NOTES
(Your notes here)
================================================================
[WP-05] INGEST LITERATURE TABLES (2019–2024)
GOAL
Bring in benchmark tables and parameter specs with full citations.
FILES TO CREATE/UPDATE
src/ingest/literature_ingest.py
data/raw/literature/
docs/citations.bib
ACTIONS
Normalize table columns to schema; set citation_key and license_tag.
Keep PDF metadata (no raw PDFs in repo if restricted).
DELIVERABLES
data/interim/staging/literature_records.jsonl
ACCEPTANCE CHECK
Each record includes citation_key and license_tag; schema valid.
NOTES
(Your notes here)
================================================================
[WP-06] INDUSTRY INGESTION TEMPLATE (NDA-SAFE)
GOAL
Define a safe, minimal format partners can drop without risk.
FILES TO CREATE/UPDATE
src/ingest/industry_ingest_template.md
data/raw/industry/README.md
ACTIONS
Specify column names, redaction/hashing guidance, and encryption at rest.
Provide a tiny dummy example JSONL with redacted placeholders.
DELIVERABLES
data/interim/staging/industry_records.jsonl (dummy)
ACCEPTANCE CHECK
Template referenced in docs/data-governance.md#industry-intake
NOTES
(Your notes here)
================================================================
[WP-07] VALIDATE & BUILD MASTER CATALOG
GOAL
Run validators, unify sources, and write the authoritative table.
FILES TO CREATE/UPDATE
src/validate/validators.py (implement checks)
data/processed/master_catalog.parquet
src/eval/reports/validation.md
ACTIONS
Checks: schema conformity, range sanity (sizes, timings), dedupe by keys, alias resolution success.
Merge records from staging sources into a single master_catalog.
DELIVERABLES
data/processed/master_catalog.parquet
src/eval/reports/validation.md with counts and error summaries.
ACCEPTANCE CHECK
Zero schema errors; duplicates < 1% and documented.
NOTES
(Your notes here)
================================================================
[WP-08] BUILD STATIC FEATURES
GOAL
Generate size/ratio/log features reproducibly.
FILES TO CREATE/UPDATE
src/features/build_static_features.py
src/features/feature_registry.yaml
data/processed/features_static.parquet
docs/feature-dictionary.md
ACTIONS
Implement raw/log-scaled sizes, size ratios (key:ct:signature), and simple heuristics.
Register every feature name/dtype/description in feature_registry.yaml.
DELIVERABLES
features_static.parquet and a human-readable dictionary.
ACCEPTANCE CHECK
Deterministic outputs for all rows in master_catalog.
NOTES
(Your notes here)
================================================================
[WP-09] BUILD DYNAMIC FEATURES
GOAL
Compute timing/memory/throughput features and scaling ratios.
FILES TO CREATE/UPDATE
src/features/build_dynamic_features.py
data/processed/features_dynamic.parquet
ACTIONS
Handle missingness gracefully (flags + imputation strategy).
Derive performance efficiency ratios (ops per byte, etc.).
DELIVERABLES
features_dynamic.parquet and a coverage table (rows with/without dynamics).
ACCEPTANCE CHECK
Missingness behavior documented in docs/dataset-limitations.md.
NOTES
(Your notes here)
================================================================
[WP-10] MERGE FEATURES → features_merged
GOAL
Join static + dynamic features and freeze a training snapshot.
FILES TO CREATE/UPDATE
data/processed/features_merged.parquet
src/eval/reports/eda.md
docs/dataset-limitations.md (append snapshot & gaps)
ACTIONS
Left-join on primary keys, assert no duplicate keys.
Summarize missingness, row/column counts.
DELIVERABLES
features_merged.parquet + EDA report.
ACCEPTANCE CHECK
Stable schema; no duplicate keys; EDA report present.
NOTES
(Your notes here)
================================================================
[WP-11] BASELINES + CV HARNESS
GOAL
Honest, calibrated baselines for Family and Level with robust splits.
FILES TO CREATE/UPDATE
src/models/train_baselines.py
src/eval/cross_validation.py
src/models/params/logreg.yaml, rf.yaml, lgbm.yaml
docs/model-cards/family_classifier.md
docs/model-cards/level_classifier.md
src/eval/reports/baselines.md
ACTIONS
Implement stratified k-fold (family-aware) and leave-one-algorithm-out.
Record accuracy/precision/recall/F1 and confusion matrices.
DELIVERABLES
baselines.md with tables and split definitions; model card baseline sections.
ACCEPTANCE CHECK
Reproducible metrics and splits; seeds recorded in reports.
NOTES
(Your notes here)
================================================================
[WP-12] ROBUSTNESS + CALIBRATION
GOAL
Evaluate reliability under noise, missing features, and plot calibration.
FILES TO CREATE/UPDATE
src/eval/robustness.py
src/eval/calibration.py
src/eval/reports/robustness.md
src/eval/reports/calibration.md
ACTIONS
Noise injection (Gaussian/uniform/structured), missing feature masks, reliability diagrams, Brier score.
DELIVERABLES
Thresholds + pass/fail gates documented in model cards.
ACCEPTANCE CHECK
Brier score target and rationale captured; failure modes documented.
NOTES
(Your notes here)
================================================================
[WP-13] INTERPRETABILITY (SHAP + LIME)
GOAL
Produce global and local explanations for both tasks.
FILES TO CREATE/UPDATE
src/interpret/shap_analysis.py
src/interpret/lime_analysis.py
src/eval/reports/interpretability.md
ACTIONS
Compute global SHAP ranking; include per-prediction LIME examples.
Sanity checks that top features align with domain expectations.
DELIVERABLES
Plots + narrative explaining feature effects.
ACCEPTANCE CHECK
Explanations are stable across seeds and splits; documented.
NOTES
(Your notes here)
================================================================
[WP-14] API SPEC + SERVICE SKELETON
GOAL
Define and stub inference API for the dashboard.
FILES TO CREATE/UPDATE
docs/api-spec.md
src/api/service.py
src/api/schemas.py
infra/Dockerfile
infra/docker-compose.yml
docs/api-examples/ (sample requests/responses)
ACTIONS
Specify endpoints: /classify, /batch, /status, /healthz.
Add versioning, error model, and example payloads.
DELIVERABLES
Mocked service that returns shaped responses.
ACCEPTANCE CHECK
docker-compose (later) will expose /healthz; examples compile.
NOTES
(Your notes here)
================================================================
[WP-15] DASHBOARD SCAFFOLD (FRONTEND + BACKEND)
GOAL
Stand up the shell UI and backend wrapper.
FILES TO CREATE/UPDATE
dashboard/frontend/package.json
dashboard/frontend/src/ (routes: Upload, Results, History)
dashboard/backend/requirements.txt
docs/dashboard-ux.md
ACTIONS
Frontend shows mock classification cards with confidence and feature-importance placeholders.
Document navigation flow and empty states.
DELIVERABLES
Basic routing + mock data render.
ACCEPTANCE CHECK
All routes load; placeholders visible; UX doc updated.
NOTES
(Your notes here)
================================================================
[WP-16] BATCH AUDIT + EXPORTABLE REPORTS
GOAL
Define inventory batch format and compliance summary exports.
FILES TO CREATE/UPDATE
docs/batch-format.md
scripts/generate_reports.sh
src/eval/reports/compliance-template.md
ACTIONS
Specify CSV/JSON input schema for bulk classification.
Provide a template for compliance summaries (flags, non-compliant, deprecated).
DELIVERABLES
Example end-to-end mocked batch documented in docs.
ACCEPTANCE CHECK
One mocked batch run described with expected outputs.
NOTES
(Your notes here)
================================================================
[WP-17] CI/CD + EXPERIMENT TRACKING (STUBS)
GOAL
Put lint/test/data/model workflow stubs and run-tracking guidance in place.
FILES TO CREATE/UPDATE
ci/github/workflows/lint-test.yml
ci/github/workflows/data-pipeline.yml
ci/github/workflows/model-train.yml
docs/experiments.md
ACTIONS
Define job names and commands (they can be no-ops initially).
Document how runs are tagged, seeded, and recorded.
DELIVERABLES
Workflows that call documented commands (even as placeholders).
ACCEPTANCE CHECK
All workflow files parse; docs/experiments.md shows the run template.
NOTES
(Your notes here)
================================================================
[WP-18] PAPER OUTLINE + ARTIFACT PLAN
GOAL
Lock paper sections, figure/table map, and artifact release plan.
FILES TO CREATE/UPDATE
docs/paper-outline.md
docs/artifacts-plan.md
ACTIONS
Map each figure/table to an existing or planned report.
Define dataset slices, licensing, and redaction policy for release.
DELIVERABLES
Checklists aligned to milestones M1/M2/M3.
ACCEPTANCE CHECK
Every planned figure/table has a source script/report mapped.
NOTES
(Your notes here)
================================================================
[WP-19] RISKS, ETHICS, AND UI SAFEGUARDS
GOAL
Finalize misuse mitigations and surface them in the UI.
FILES TO CREATE/UPDATE
docs/ethics.md (expanded)
docs/risk-register.md (adversarial/misuse entries)
docs/dashboard-ux.md (warning banners, thresholds, copy)
ACTIONS
Add “not a security proof” guidance and confidence threshold defaults.
Clarify escalation paths for ambiguous results.
DELIVERABLES
Final UI copy blocks for warnings/limits.
ACCEPTANCE CHECK
UX doc includes screenshots/wireframes or text mockups of warnings.
NOTES
(Your notes here)
================================================================
[WP-20] DATASET EXPANSION & QUARTERLY CADENCE
GOAL
Codify post-training data integration and refresh rhythm.
FILES TO CREATE/UPDATE
docs/dataset-limitations.md (Expansion Strategy & CI Pulls)
docs/data-governance.md (Quarterly refresh SOP)
docs/timeline.md (cadence milestones)
docs/data-sources-table.md (target counts by source)
ACTIONS
Define target scale: ≥1000 implementations, ≥50 parameter sets.
Specify retrain triggers (delta in coverage, drift, or partner drops).
DELIVERABLES
Data-sources table with targets and actuals.
ACCEPTANCE CHECK
Clear re-training criteria and changelog requirements listed.
NOTES
(Your notes here)
================================================================
GENERAL EXECUTION HABITS (LESSONS APPLIED)
Lock names/paths early (config/paths.yaml) so scripts and docs match.
Keep tiny, validated examples (toy records, mocked batches) next to specs.
Write one-page “what success looks like” in each docs/* report so you can compare runs without digging into code.
Prefer deterministic seeds and record them in every report.
When a check fails, fail loud: write a short .md with the error summary into src/eval/reports/ and link it from CHANGELOG.md.
================================================================
READY COMMANDS YOU’LL LIKELY USE LATER (NAMES ONLY FOR NOW)
scripts/make_dataset.sh
scripts/run_cv.sh
scripts/generate_reports.sh
docker-compose up
pytest (once tests exist)
ruff/black (or your preferred linters/formatters)
================================================================
END OF FILE